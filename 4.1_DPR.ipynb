{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JHU JSALT Summer School IR Laboratory -- Part 4.1\n",
        "\n",
        "In this notebook, we are going to walk through a CLIR example using a DPR model on the NeuCLIR Chinese collection."
      ],
      "metadata": {
        "id": "EbpwWjowpY9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Started"
      ],
      "metadata": {
        "id": "4SGg5BV7p0ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell will check whether this notebook has GPU access. Upon execution you should see a table with Nvidia GPU information. If you are seeing an command error, that means you are either running a CPU or TPU VM. In this case, you should switch to a GPU using Runtime > Change runtime type."
      ],
      "metadata": {
        "id": "j0Ic15Hyp38l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7BDRxfPp4pb",
        "outputId": "9aa82eab-0265-4fa7-e4c9-a05d16d72fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 16 19:35:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install the packages!\n",
        "The following command will install `ir_measurees`, Huggingface `datasets`, Google Translate (for presentation), and Huggingface Transformers."
      ],
      "metadata": {
        "id": "sQu0uiT3p996"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SThe-31BVe6W",
        "outputId": "ba170335-dabb-45f9-b4ec-a3fe043e8409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ir_measures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cwl-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U --progress-bar on ir_measures transformers datasets googletrans==3.1.0a0 more-itertools faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installation, let's download the dataset. The [NeuCLIR 1 Collection](https://huggingface.co/datasets/neuclir/neuclir1) is publicly available on Huggingface Datasets! Topics and qrels are available on the TREC website, from which we will directly download it.\n",
        "\n",
        "We are going to rerank a baseline BM25 search result provided by the NeuCLIR organizers; this is also available on the TREC website.\n",
        "\n",
        "However, working with the entire NeuCLIR Chinese collection will take too much indexing time. For this demonstration, we'll just use the first 40k documents."
      ],
      "metadata": {
        "id": "tUXMFvLNqGpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download topics and qrels from NIST\n",
        "!wget -q --show-progress https://trec.nist.gov/data/neuclir/topics.0720.utf8.jsonl\n",
        "!wget -q --show-progress https://trec.nist.gov/data/neuclir/2022-qrels.zho\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import ir_measures as irms\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Only loading the first 40k docs from HF Datasets\n",
        "ds = load_dataset('neuclir/neuclir1', split='zho', streaming=True) # total 3179209\n",
        "doc_subset = [ o for i, o in zip(tqdm(range(40_000), desc='Loading first 40k docs from NeuCLIR Chinese Collection'), ds) ]\n",
        "subset_doc_ids = set([ d['id'] for d in doc_subset ])\n",
        "\n",
        "use_topic = '66' # use topic 66 as demo -- expecting to have 9 relevant docs\n",
        "\n",
        "qrels = pd.DataFrame([ l for l in irms.read_trec_qrels('2022-qrels.zho') if l.query_id == use_topic and l.doc_id in subset_doc_ids ])\n",
        "topics = [ t for t in map(json.loads, open(\"topics.0720.utf8.jsonl\")) if t['topic_id'] == use_topic ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "f5e68ef9a8ad453fa87ddaa99717c289",
            "0acd9bf24ac442abb8c6ffb5e00d570f",
            "2dac977d93d149a3b2f85fcb24523015",
            "71092943c3504adcbe846b4acb86051d",
            "4d3b3fa743f74b379d8f3b31224990a5",
            "4e93a4f1b81d4291ad1a5fb8c5226637",
            "1c68e3cf80704db0b59971250f5f5f9f",
            "ce88aefe4d194f3386e8288564f6465a",
            "cf619cd61d1245778811cafe23405038",
            "f7a7862a115446128536029d965e1b0b",
            "f602d0e8a6694e229d853c634787e2ee"
          ]
        },
        "id": "8TvD-H3ojKjf",
        "outputId": "5a60f58f-9e8f-4bf0-b484-d9c41f958d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topics.0720.utf8.js 100%[===================>] 646.75K   502KB/s    in 1.3s    \n",
            "2022-qrels.zho.1    100%[===================>]   1.54M   921KB/s    in 1.7s    \n",
            "zho-base-run-result 100%[===================>]   9.11M   691KB/s    in 22s     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading first 40k docs from NeuCLIR Chinese Collection:   0%|          | 0/40000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5e68ef9a8ad453fa87ddaa99717c289"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create helper functions so we can obtain the query and document text more conveniently."
      ],
      "metadata": {
        "id": "NCkAXqfbqerT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_id_idx = { t['topic_id']: i for i, t in enumerate(topics) }\n",
        "def get_query_by_topic_id(topic_id, query_type='title'):\n",
        "    return topics[ topic_id_idx[topic_id] ]['topics'][0][f'topic_{query_type}']\n",
        "\n",
        "doc_id_to_idx = { d['id']: i for i, d in enumerate(doc_subset) }\n",
        "def get_doc_text_by_doc_id(doc_id):\n",
        "    doc = doc_subset[ doc_id_to_idx[doc_id] ]\n",
        "    return doc['title'] + ' ' + doc['text']"
      ],
      "metadata": {
        "id": "pbguuFpklC1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Retrieval with One Vector Per Sequence\n",
        "\n",
        "Now we are ready to start working on encoding our documents.\n",
        "\n",
        "Let's first load a model that was pretrained with text in multiple languages and capable of encoding the meaning of the text into a vector.\n",
        "\n",
        "In this example, we use `eugene-yang/dpr-xlm-align-C3-zhen` but there are a lot of models out there that can do job."
      ],
      "metadata": {
        "id": "k4MKxGhXd_JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import faiss\n",
        "\n",
        "from more_itertools import batched\n",
        "\n",
        "model_name = 'eugene-yang/dpr-xlm-align-C3-zhen'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model = model.half().to('cuda')\n"
      ],
      "metadata": {
        "id": "6QWq1OrdYH2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the model, let's define the `mean_pooling` function suggested by [`sentence-transformers`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2). The pooling function varies from model to model. You should be aware of which one you should be using when using a pretrained model."
      ],
      "metadata": {
        "id": "E-RFpfoYei7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
      ],
      "metadata": {
        "id": "c0wsJ9_QXyEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to support fast searching, which we can rely on `faiss` for vector nearest neighbor search. In this demo, we use a index setup that performs exact matches, which is slow for large collections. If you would like to use something that's faster, you could use other setups, e.g. `PQ384x4fs`. See https://github.com/facebookresearch/faiss/wiki/The-index-factory for more.\n",
        "\n",
        "The first argument a faiss index needs is the dimension of the vectors, which we can get by doing a mini test run with an empty string.\n",
        "\n",
        "Here, since `faiss` only maintain a running integer index of the vectors, we need to manually maintain a mapping between the running id and the actual document ids. You could also use `IndexIDMap` provided by `faiss` but for simplicity, we just use a python list here.\n",
        "\n"
      ],
      "metadata": {
        "id": "HVR55Q_oe4Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    dimension = model(**tokenizer(\"\", return_tensors='pt').to(model.device)).last_hidden_state.shape[-1]\n",
        "print(dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fsqozos1Ttt",
        "outputId": "74ac633c-992b-434d-ce53-c002206e19e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.index_factory(dimension, \"Flat\")\n",
        "docid_mapping = []"
      ],
      "metadata": {
        "id": "RbGf_9a3akbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to encode the documents.\n",
        "\n",
        "Here we use a very simple data loader that batch the documents. You can certainly use a PyTorch Dataloader but just to keep things simple here.\n",
        "\n",
        "A tricks with working with padding is to sort the input text from short to long, so that you don't apply excessive paddings to batches with shorter text."
      ],
      "metadata": {
        "id": "5BvNdBEV4DVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "loader = tqdm(\n",
        "    batched(sorted(map(lambda x: (x['id'], f\"{x['title']}\\n{x['text']}\"), doc_subset), key=lambda x: len(x[1])),\n",
        "            batch_size),\n",
        "    total=len(doc_subset)/batch_size\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in loader:\n",
        "        docids, text = list(zip(*batch))\n",
        "        inputs = tokenizer(text, padding='longest', max_length=512, truncation=True, return_tensors='pt')\n",
        "        model_output = model(**inputs.to(model.device))\n",
        "        # text_embeddings = mean_pooling(model_output, inputs['attention_mask']).cpu()\n",
        "        text_embeddings = model_output.last_hidden_state[:, 0].cpu()\n",
        "\n",
        "        index.add(text_embeddings.numpy().astype('float32'))\n",
        "        docid_mapping += docids\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "811aa433d35f4641bbb723049dca3ff6",
            "9564a51fc0df415db04a1f4c6395af94",
            "8c83628f1b27439c864823fe0a6c16c7",
            "7d4db635d5c641b3a803871a06e06653",
            "abd09100b0f84e4b9686024fb4018dd3",
            "1d8930cbc23340b88e2788388ccc5609",
            "88a4792d5bc042af8aa02f1f326928a8",
            "03c8cb4fbe924a389395dd2a8a3a64b7",
            "afdcc62367f34283b3ced27af3269ffd",
            "2582e979c7c448cc8ddad987b67070e1",
            "6be9586199264ce89500dffbfd5d166f"
          ]
        },
        "id": "6-L4jowPYiqY",
        "outputId": "9ad03d9e-5f7a-4027-e6d9-59b6c84ca05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/156.25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "811aa433d35f4641bbb723049dca3ff6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can verify how many vectors we have added to the index by using the following command."
      ],
      "metadata": {
        "id": "EKcfbdZM4c8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.ntotal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKArcW35fsJm",
        "outputId": "59c26e86-d341-420e-e424-97d70515a8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we are ready for some searching!\n",
        "We first need to get the query text and encode them using the same process."
      ],
      "metadata": {
        "id": "Am8JJIPC4jz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = get_query_by_topic_id(use_topic) + ' ' + get_query_by_topic_id(use_topic, query_type='description')\n",
        "\n",
        "print(query_text)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer(query_text, padding=True, truncation=True, return_tensors='pt')\n",
        "    query_embeddings = mean_pooling(model(**inputs.to(model.device)), inputs['attention_mask']).cpu()\n",
        "\n",
        "print(query_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eAW-qvLf2mg",
        "outputId": "c78c8f12-6756-4f2a-e219-477bdb21df83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COVID-19 vaccination rate in China I am interested in finding articles that provide information about the COVID-19 vaccination rate in China.\n",
            "torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can search the `faiss` index with the query embeddings.\n",
        "It returns two `numpy` arrays, the similarity values the their associated running index."
      ],
      "metadata": {
        "id": "MJkmMU484w97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D, I = index.search(query_embeddings.numpy(), k=1000)"
      ],
      "metadata": {
        "id": "UfYjQwqigXo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then map them to the original document id by using the `docid_mapping` that we maintained during indexing.\n",
        "\n",
        "And finally, we can evaluate the performance."
      ],
      "metadata": {
        "id": "4kGU6oaC4wnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking = {\n",
        "    docid_mapping[int(i)]: float(v)\n",
        "    for i, v in zip(I[0], -D[0])\n",
        "}\n",
        "\n",
        "irms.calc_aggregate([irms.nDCG@20, irms.AP, irms.R@100], qrels, {use_topic: ranking})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYj7swI7ggDV",
        "outputId": "3af6f48d-4ee9-4a07-d286-1da16223e886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{nDCG@20: 0.07075576072108061,\n",
              " R@100: 0.5555555555555556,\n",
              " AP: 0.05835123228926886}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, it's not so great isn't it! However, it has some decent recall that we can use with a reranker (or most of them time you would just want to use a BM25...)."
      ],
      "metadata": {
        "id": "xv3H8FUlsNHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "Well, you might have already realized that the tokenizer was truncating the text with a maximum length of 512. We can actually do better then that!\n",
        "\n",
        "There's a commonly-used techinque called `MaxP`, which splits the documents into passages, search the passage collection, and use the maximunm passage score of a document as its document score.\n",
        "\n",
        "See more: https://arxiv.org/pdf/1905.09217\n",
        "\n",
        "Let's implement this and see if it actually improves the result!"
      ],
      "metadata": {
        "id": "unnocDTp0jJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution\n",
        "\n"
      ],
      "metadata": {
        "id": "FhwxFB4S0jvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title See Solution\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "def split_documents(doc, length=180, stride=180):\n",
        "    doc_text = f\"{doc['title']}\\n{doc['text']}\"\n",
        "    tokens = tokenizer.encode(doc_text, add_special_tokens=False)\n",
        "    for i, offset in enumerate(range(0, len(tokens), stride)):\n",
        "        yield (doc['id'], i, tokenizer.decode(tokens[offset:offset+length]))\n",
        "\n",
        "batch_size = 256\n",
        "estimate_npassages = sum([ len(d['title'] + d['text'])//180+1 for d in doc_subset ])\n",
        "loader = tqdm(\n",
        "    batched(chain.from_iterable(map(split_documents, doc_subset)), batch_size),\n",
        "    total=estimate_npassages//batch_size\n",
        ")\n",
        "\n",
        "maxp_index = faiss.index_factory(dimension, \"Flat\")\n",
        "maxp_docid_mapping = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in loader:\n",
        "        docids, passage_ids, text = list(zip(*batch))\n",
        "        inputs = tokenizer(text, padding='longest', max_length=512, truncation=True, return_tensors='pt')\n",
        "        model_output = model(**inputs.to(model.device))\n",
        "        # text_embeddings = mean_pooling(model_output, inputs['attention_mask']).cpu()\n",
        "        text_embeddings = model_output.last_hidden_state[:, 0].cpu()\n",
        "\n",
        "        maxp_index.add(text_embeddings.numpy().astype('float32'))\n",
        "        maxp_docid_mapping += docids\n",
        "\n",
        "# since we are using the same model, let me reuse the query embeddings\n",
        "D, I = maxp_index.search(query_embeddings.numpy(), k=1000)\n",
        "\n",
        "# MaxP\n",
        "maxp_ranking = {}\n",
        "for idx, val in zip(I[0], -D[0]):\n",
        "    docid = maxp_docid_mapping[idx]\n",
        "    if docid not in maxp_ranking or maxp_ranking[docid] < val:\n",
        "        maxp_ranking[docid] = float(val)\n",
        "\n",
        "irms.calc_aggregate([irms.nDCG@20, irms.AP, irms.R@100], qrels, {use_topic: maxp_ranking})"
      ],
      "metadata": {
        "id": "ViXvJDnPz4rg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And there you go!\n",
        "\n",
        "You've learned how to run a DPR model for CLIR!"
      ],
      "metadata": {
        "id": "JJTZYonps0HA"
      }
    }
  ]
}